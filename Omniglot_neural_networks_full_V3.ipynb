{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Omniglot_neural_networks_full_V3.ipynb","provenance":[{"file_id":"1w_elyHY_ibyEqFQ5-GJ28TkwAxCLVaEt","timestamp":1619278467992},{"file_id":"1fNzdMzJmbsnnqjqrV9W-HnPrZ9MPtlsy","timestamp":1616926115423}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uEssHHPaUcRD"},"source":["# Работа с полным датасетом Omniglot\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w9QxlSQ_UPQ0"},"source":["# Загрузка полного датасета Omniglot"]},{"cell_type":"markdown","metadata":{"id":"8delerMRUQRb"},"source":["Загрузка датасета"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-B2DrFZtPZfZ","executionInfo":{"status":"ok","timestamp":1621494000605,"user_tz":-180,"elapsed":247091,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}},"outputId":"155824e3-6b98-42e6-d84d-e0b4702cfa1a"},"source":["from sklearn.metrics import accuracy_score\n","from google.colab import drive\n","\n","import numpy as np\n","import zipfile\n","import io\n","import os\n","\n","NUMBER_OF_CLASSES = 1600\n","PATH = 'drive/My Drive/Few-Shot-Learning/Omniglot-data-set'\n","\n","drive.mount('/content/drive')\n","\n","with zipfile.ZipFile(os.path.join(PATH, 'train_images.zip')) as zipper:\n","    with io.BufferedReader(zipper.open('train_images.txt', mode='r')) as file:\n","        X_train = np.resize(np.loadtxt(file, dtype=np.uint8), (14 * NUMBER_OF_CLASSES, 1, 105, 105))\n","        \n","with zipfile.ZipFile(os.path.join(PATH, 'train_labels.zip')) as zipper:\n","    with io.BufferedReader(zipper.open('train_labels.txt', mode='r')) as file:\n","        y_train = np.resize(np.loadtxt(file, dtype='str'), (14 * NUMBER_OF_CLASSES))\n","        \n","with zipfile.ZipFile(os.path.join(PATH, 'test_images.zip')) as zipper:\n","    with io.BufferedReader(zipper.open('test_images.txt', mode='r')) as file:\n","        X_test = np.resize(np.loadtxt(file, dtype=np.uint8), (6 * NUMBER_OF_CLASSES, 1, 105, 105))\n","        \n","with zipfile.ZipFile(os.path.join(PATH, 'test_labels.zip')) as zipper:\n","    with io.BufferedReader(zipper.open('test_labels.txt', mode='r')) as file:\n","        y_test = np.resize(np.loadtxt(file, dtype='str'), (6 * NUMBER_OF_CLASSES))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XqIB2axQGeD9"},"source":["Определение трансформации"]},{"cell_type":"code","metadata":{"id":"VR9Bg_Rj-MkI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621494013026,"user_tz":-180,"elapsed":243051,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}},"outputId":"8c85798e-9c8a-4a64-c0a6-827cc0403e70"},"source":["!pip install -q -U albumentations\n","import albumentations as A\n","\n","from albumentations.pytorch import ToTensorV2\n","\n","transform = A.Compose([A.SmallestMaxSize(max_size=105),\n","                       A.CenterCrop(height=84, width=84),\n","                       A.Normalize(mean=(0.5), std=(0.5)),\n","                       ToTensorV2(),])"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 81kB 6.1MB/s \n","\u001b[K     |████████████████████████████████| 38.2MB 77kB/s \n","\u001b[K     |████████████████████████████████| 952kB 47.9MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H3whmknTOixN"},"source":["Определение тензоров"]},{"cell_type":"code","metadata":{"id":"Z_h_uzVOKDpj","executionInfo":{"status":"ok","timestamp":1621494016491,"user_tz":-180,"elapsed":244881,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}}},"source":["import torch\n","\n","train_images = torch.stack([transform(image=x[0])[\"image\"] for x in X_train]).float()\n","test_images = torch.stack([transform(image=x[0])[\"image\"] for x in X_test]).float()\n","\n","classes = [y_train[14 * i] for i in range(NUMBER_OF_CLASSES)]\n","\n","train_labels = torch.from_numpy(np.array([[i] for i in range(NUMBER_OF_CLASSES) for _ in range(14)])).long()\n","test_labels = torch.from_numpy(np.array([[i] for i in range(NUMBER_OF_CLASSES) for _ in range(6)])).long()"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaB00EAcNpXr"},"source":["Определение вспомогательной функции для подготовки выборки"]},{"cell_type":"code","metadata":{"id":"SEBIgF0SNplC","executionInfo":{"status":"ok","timestamp":1621494016492,"user_tz":-180,"elapsed":244098,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}}},"source":["def shuffle_tensor(images, labels, batch_size, count=14):\n","    perm = torch.randperm(count * NUMBER_OF_CLASSES)\n","\n","    images = images[perm].view(count * NUMBER_OF_CLASSES // batch_size, batch_size, 1, 84, 84).float()\n","    labels = labels[perm].view(count * NUMBER_OF_CLASSES // batch_size, batch_size).long()\n","\n","    return list(zip(images, labels))\n","    \n","\n","def shuffle_tensor_tuple(images, labels, batch_size, count=14):\n","    perm = torch.randperm(count * NUMBER_OF_CLASSES)\n","\n","    images = images[perm].view(count * NUMBER_OF_CLASSES // 2 // batch_size, 2, batch_size, 1, 84, 84).float()\n","    labels = labels[perm].view(count * NUMBER_OF_CLASSES // 2 // batch_size, 2, batch_size).long()\n","\n","    return list(zip(images, labels))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0ZYiV7ONxIN"},"source":["# Визуализация датасета Omniglot (полный датасет)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"khxuQvFPNxza","executionInfo":{"status":"ok","timestamp":1621494016493,"user_tz":-180,"elapsed":241994,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}},"outputId":"21825952-01bd-4a0f-ced5-c420366d9d2e"},"source":["from PIL import Image\n","\n","digit_number = -1\n","\n","image = Image.fromarray(X_train[digit_number].reshape(105, 105), mode='P')\n","\n","print('label: {}'.format(y_train[digit_number]))\n","display(image.resize((315, 315)))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["label: ULOG_03\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<PIL.Image.Image image mode=P size=315x315 at 0x7F32EBFDFDD0>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"OAdozHcOpiTH"},"source":["# Использование сиамской нейронной сети для решения задачи классификации рукописных символов (полный датасет)"]},{"cell_type":"markdown","metadata":{"id":"mmGoswhmpiTR"},"source":["Создание сиамской нейронной сети"]},{"cell_type":"code","metadata":{"id":"Wdw_iGsxwDPn","executionInfo":{"status":"ok","timestamp":1621494016493,"user_tz":-180,"elapsed":240164,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 16, 3)\n","        self.conv2 = nn.Conv2d(16, 32, 3)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.bnorm1 = nn.BatchNorm2d(16)\n","        self.bnorm2 = nn.BatchNorm2d(32)\n","        self.drop = nn.Dropout(p=0.2)\n","        self.fc1 = nn.Linear(32 * 19 * 19, 3600)\n","        self.fc2 = nn.Linear(3600, 480)\n","\n","    def forward_once(self, x):\n","        x_out = F.relu(self.bnorm1(self.pool(self.conv1(x))))\n","        x_out = F.relu(self.bnorm2(self.pool(self.conv2(x_out))))\n","        x_out = x_out.view(-1, 32 * 19 * 19)\n","        x_out = F.relu(self.drop(self.fc1(x_out)))\n","        x_out = self.fc2(x_out)\n","        return x_out\n","\n","    def forward(self, x_1, x_2):\n","        return self.forward_once(x_1), self.forward_once(x_2)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AabMz3s84BCd"},"source":["Определение функции потерь"]},{"cell_type":"code","metadata":{"id":"Xhvou34IyHk_","executionInfo":{"status":"ok","timestamp":1621494016494,"user_tz":-180,"elapsed":238924,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}}},"source":["class ContrastiveLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(ContrastiveLoss, self).__init__()\n","\n","        self.margin = margin\n","\n","    def forward(self, x_1, x_2, y):\n","        diff = x_1 - x_2\n","        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n","        dist = torch.sqrt(dist_sq)\n","\n","        mdist = self.margin - dist\n","        dist = torch.clamp(mdist, min=0.0)\n","\n","        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n","        loss = torch.sum(loss) / 2.0 / x_1.size()[0]\n","\n","        return loss"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJ9M01kWpiTT"},"source":["Обучение сиамской нейронной сети"]},{"cell_type":"code","metadata":{"id":"qRWgG6LYpiTT"},"source":["import torch.optim as optim\n","\n","net = Net().cuda()\n","\n","criterion = ContrastiveLoss(margin=800)\n","optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=0.0005)\n","\n","net.train()\n","\n","for iteration in range(10000):\n","    trainset = shuffle_tensor_tuple(train_images, train_labels, 350)\n","\n","    for data in trainset:\n","        images, labels = data[0].cuda(), data[1].cuda()\n","\n","        optimizer.zero_grad()\n","\n","        labels_pred = net(*images)\n","        loss = criterion(*labels_pred, (labels[0] == labels[1]).type(torch.uint8))\n","\n","        loss.backward()\n","        optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HUE63YV1piTU"},"source":["Использование сиамской нейронной сети"]},{"cell_type":"code","metadata":{"id":"NkrwIawIu7o2","executionInfo":{"status":"ok","timestamp":1621518727065,"user_tz":-180,"elapsed":197627,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}}},"source":["def distance(x_1, x_2):\n","    return np.sqrt(np.sum((x_1 - x_2) ** 2, axis=1))\n","\n","net.eval()\n","\n","X_train_encoded = []\n","y_train_encoded = []\n","\n","trainset = shuffle_tensor(train_images, train_labels, 350)\n","\n","for images, labels in trainset:\n","    X_train_encoded.append(net.forward_once(images.cuda()).cpu().detach().numpy())\n","    y_train_encoded.append(labels.detach().numpy())\n","\n","X_train_encoded = np.reshape(np.array(X_train_encoded), (-1, 480))\n","y_train_encoded = np.reshape(np.array(y_train_encoded), (-1))\n","\n","\n","X_test_encoded = []\n","y_test_encoded = []\n","\n","testset = shuffle_tensor(test_images, test_labels, 300, 6)\n","\n","for images, labels in testset:\n","    X_test_encoded.append(net.forward_once(images.cuda()).cpu().detach().numpy())\n","    y_test_encoded.append(labels.detach().numpy())\n","\n","X_test_encoded = np.reshape(np.array(X_test_encoded), (-1, 480))\n","y_test_encoded = np.reshape(np.array(y_test_encoded), (-1))\n","\n","\n","predicted_s = []\n","\n","for encoded in X_test_encoded:\n","    dists = distance(encoded, X_train_encoded)\n","    index = dists.argmin()\n","\n","    predicted_s.append(y_train_encoded[index])"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJK0QXn0piTV"},"source":["Вычисление метрик"]},{"cell_type":"code","metadata":{"id":"zGqq25fDpiTV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621518727066,"user_tz":-180,"elapsed":197623,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}},"outputId":"cad4c425-da00-428e-a483-2f6bb03392ba"},"source":["print('accuracy {:.3}'.format(accuracy_score(y_test_encoded, predicted_s)))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["accuracy 0.482\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qFGQc7rFIbMV"},"source":["# Сравнение результатов работы сиамской нейронной сетей с рассмотренными ранее нейронными сетями для полного датасета Omniglot (полный датасет)"]},{"cell_type":"code","metadata":{"id":"E8eSXXkJcm-H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621518727067,"user_tz":-180,"elapsed":197619,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"07541017626097830699"}},"outputId":"d23491cf-672b-42a7-d794-af1c502302ea"},"source":["from sklearn.metrics import accuracy_score\n","\n","print('Siamese Neural network accuracy: {:.3}'.format(accuracy_score(y_test_encoded, predicted_s)))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Siamese Neural network accuracy: 0.482\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pUiuyl4jJQtO"},"source":["При переходе к полному датасету число классов увеличилось, но число обучающих объектов каждого класса осталось неизменным. Это привело к проблеме низкой вероятности попадания объектов одного класса в пару, которую пришлось решать новым подбором гиперпараметра отступа и увеличением числа эпох для обучения нейронной сети. Если сравнивать с уже рассмотренными архитектурами нейронных сетей, то данный подход хоть и позволил обогнать по метрике точности полносвязную и сверточную нейронные сети, но достичь результатов данных сетей с применением аугментации не представляется возможным. В целом такой результат показателя метрики точности, учитывая число классов данного примера задачи классификации, является более чем достойным."]}]}