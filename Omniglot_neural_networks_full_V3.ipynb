{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Omniglot_neural_networks_full_V3.ipynb","provenance":[{"file_id":"1w_elyHY_ibyEqFQ5-GJ28TkwAxCLVaEt","timestamp":1619278467992},{"file_id":"1fNzdMzJmbsnnqjqrV9W-HnPrZ9MPtlsy","timestamp":1616926115423}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uEssHHPaUcRD"},"source":["# Работа с полным датасетом Omniglot\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w9QxlSQ_UPQ0"},"source":["# Загрузка полного датасета Omniglot"]},{"cell_type":"markdown","metadata":{"id":"8delerMRUQRb"},"source":["Загрузка датасета"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-B2DrFZtPZfZ","executionInfo":{"status":"ok","timestamp":1620890641106,"user_tz":-180,"elapsed":318127,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"13051415308691321255"}},"outputId":"c76a29b2-f3ce-4034-d23c-37fc29e77d6e"},"source":["from sklearn.metrics import accuracy_score\n","from google.colab import drive\n","\n","import numpy as np\n","import zipfile\n","import io\n","import os\n","\n","NUMBER_OF_CLASSES = 1600\n","PATH = 'drive/My Drive/Few-Shot-Learning/Omniglot-data-set'\n","\n","drive.mount('/content/drive')\n","\n","with zipfile.ZipFile(os.path.join(PATH, 'train_images.zip')) as zipper:\n","    with io.BufferedReader(zipper.open('train_images.txt', mode='r')) as file:\n","        X_train = np.resize(np.loadtxt(file, dtype=np.uint8), (14 * NUMBER_OF_CLASSES, 1, 105, 105))\n","        \n","with zipfile.ZipFile(os.path.join(PATH, 'train_labels.zip')) as zipper:\n","    with io.BufferedReader(zipper.open('train_labels.txt', mode='r')) as file:\n","        y_train = np.resize(np.loadtxt(file, dtype='str'), (14 * NUMBER_OF_CLASSES))\n","        \n","with zipfile.ZipFile(os.path.join(PATH, 'test_images.zip')) as zipper:\n","    with io.BufferedReader(zipper.open('test_images.txt', mode='r')) as file:\n","        X_test = np.resize(np.loadtxt(file, dtype=np.uint8), (6 * NUMBER_OF_CLASSES, 1, 105, 105))\n","        \n","with zipfile.ZipFile(os.path.join(PATH, 'test_labels.zip')) as zipper:\n","    with io.BufferedReader(zipper.open('test_labels.txt', mode='r')) as file:\n","        y_test = np.resize(np.loadtxt(file, dtype='str'), (6 * NUMBER_OF_CLASSES))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XqIB2axQGeD9"},"source":["Определение трансформации"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VR9Bg_Rj-MkI","executionInfo":{"status":"ok","timestamp":1620890655221,"user_tz":-180,"elapsed":308989,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"13051415308691321255"}},"outputId":"5d3595a3-2698-4463-bda9-1aacab326f0c"},"source":["!pip install -q -U albumentations\n","import albumentations as A\n","\n","from albumentations.pytorch import ToTensorV2\n","\n","transform = A.Compose([A.SmallestMaxSize(max_size=105),\n","                       A.CenterCrop(height=84, width=84),\n","                       A.Normalize(mean=(0.5), std=(0.5)),\n","                       ToTensorV2(),])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 81kB 5.8MB/s \n","\u001b[K     |████████████████████████████████| 952kB 17.4MB/s \n","\u001b[K     |████████████████████████████████| 38.2MB 73kB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H3whmknTOixN"},"source":["Определение тензоров"]},{"cell_type":"code","metadata":{"id":"Z_h_uzVOKDpj"},"source":["import torch\n","\n","train_images = torch.stack([transform(image=x[0])[\"image\"] for x in X_train]).float()\n","test_images = torch.stack([transform(image=x[0])[\"image\"] for x in X_test]).float()\n","\n","classes = [y_train[14 * i] for i in range(NUMBER_OF_CLASSES)]\n","\n","train_labels = torch.from_numpy(np.array([[i] for i in range(NUMBER_OF_CLASSES) for _ in range(14)])).long()\n","test_labels = torch.from_numpy(np.array([[i] for i in range(NUMBER_OF_CLASSES) for _ in range(6)])).long()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaB00EAcNpXr"},"source":["Определение вспомогательной функции для подготовки выборки"]},{"cell_type":"code","metadata":{"id":"SEBIgF0SNplC"},"source":["import random\n","\n","def shuffle_tensor(tensor, batch_size, count=14):\n","    random.shuffle(tensor)\n","\n","    images, labels = zip(*tensor)\n","\n","    images = torch.stack(images[:])\n","    labels = torch.stack(labels[:])\n","\n","    images = images.view(count * NUMBER_OF_CLASSES // batch_size, batch_size, 1, 84, 84).float()\n","    labels = labels.view(count * NUMBER_OF_CLASSES // batch_size, batch_size).long()\n","\n","    return list(zip(images, labels))\n","    \n","\n","def shuffle_tensor_tuple(tensor, batch_size, count=14):\n","    random.shuffle(tensor)\n","\n","    images, labels = zip(*tensor)\n","\n","    images = torch.stack(images[:])\n","    labels = torch.stack(labels[:])\n","\n","    images = images.view(count * NUMBER_OF_CLASSES // batch_size // 2, 2, batch_size, 1, 84, 84).float()\n","    labels = labels.view(count * NUMBER_OF_CLASSES // batch_size // 2, 2, batch_size).long()\n","\n","    return list(zip(images, labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0ZYiV7ONxIN"},"source":["# Визуализация датасета Omniglot (полный датасет)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"khxuQvFPNxza","executionInfo":{"status":"ok","timestamp":1620890660182,"user_tz":-180,"elapsed":310398,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"13051415308691321255"}},"outputId":"3ae2a482-2af2-4152-a39a-d88b52d53e81"},"source":["from PIL import Image\n","\n","digit_number = -1\n","\n","image = Image.fromarray(X_train[digit_number].reshape(105, 105), mode='P')\n","\n","print('label: {}'.format(y_train[digit_number]))\n","display(image.resize((315, 315)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["label: ULOG_03\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<PIL.Image.Image image mode=P size=315x315 at 0x7FAFF6348250>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"OAdozHcOpiTH"},"source":["# Использование сиамской нейронной сети для решения задачи классификации рукописных символов (полный датасет)"]},{"cell_type":"markdown","metadata":{"id":"mmGoswhmpiTR"},"source":["Создание сиамской нейронной сети"]},{"cell_type":"code","metadata":{"id":"Wdw_iGsxwDPn"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 16, 3)\n","        self.conv2 = nn.Conv2d(16, 32, 3)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.bnorm1 = nn.BatchNorm2d(16)\n","        self.bnorm2 = nn.BatchNorm2d(32)\n","        self.drop = nn.Dropout(p=0.2)\n","        self.fc1 = nn.Linear(32 * 19 * 19, 3600)\n","        self.fc2 = nn.Linear(3600, 480)\n","\n","    def forward_once(self, x):\n","        x_out = F.relu(self.bnorm1(self.pool(self.conv1(x))))\n","        x_out = F.relu(self.bnorm2(self.pool(self.conv2(x_out))))\n","        x_out = x_out.view(-1, 32 * 19 * 19)\n","        x_out = F.relu(self.drop(self.fc1(x_out)))\n","        x_out = self.fc2(x_out)\n","        return x_out\n","\n","    def forward(self, x_1, x_2):\n","        return self.forward_once(x_1), self.forward_once(x_2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AabMz3s84BCd"},"source":["Определение функции потерь"]},{"cell_type":"code","metadata":{"id":"Xhvou34IyHk_"},"source":["class ContrastiveLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super(ContrastiveLoss, self).__init__()\n","\n","        self.margin = margin\n","\n","    def forward(self, x_1, x_2, y):\n","        diff = x_1 - x_2\n","        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n","        dist = torch.sqrt(dist_sq)\n","\n","        mdist = self.margin - dist\n","        dist = torch.clamp(mdist, min=0.0)\n","\n","        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n","        loss = torch.sum(loss) / 2.0 / x_1.size()[0]\n","\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJ9M01kWpiTT"},"source":["Обучение сиамской нейронной сети"]},{"cell_type":"code","metadata":{"id":"wAr1Io6i0Tb8"},"source":["def metric(model):\n","    def distance(x_1, x_2):\n","        return np.sqrt(np.sum((x_1 - x_2) ** 2, axis=1))\n","\n","    model.eval()\n","\n","    X_train_encoded = []\n","    y_train_encoded = []\n","\n","    trainset = shuffle_tensor(list(zip(train_images, train_labels)), 350)\n","\n","    for images, labels in trainset:\n","        X_train_encoded.append(model.forward_once(images.cuda()).cpu().detach().numpy())\n","        y_train_encoded.append(labels.detach().numpy())\n","\n","    X_train_encoded = np.reshape(np.array(X_train_encoded), (-1, 480))\n","    y_train_encoded = np.reshape(np.array(y_train_encoded), (-1))\n","\n","\n","    X_test_encoded = []\n","    y_test_encoded = []\n","\n","    testset = shuffle_tensor(list(zip(test_images, test_labels)), 300, 6)\n","\n","    for images, labels in testset:\n","        X_test_encoded.append(model.forward_once(images.cuda()).cpu().detach().numpy())\n","        y_test_encoded.append(labels.detach().numpy())\n","\n","    X_test_encoded = np.reshape(np.array(X_test_encoded), (-1, 480))\n","    y_test_encoded = np.reshape(np.array(y_test_encoded), (-1))\n","\n","\n","    predicted_s = []\n","\n","    for encoded in X_test_encoded:\n","        dists = distance(encoded, X_train_encoded)\n","        index = dists.argmin()\n","\n","        predicted_s.append(y_train_encoded[index])\n","\n","    model.train()\n","\n","    print('accuracy {:.3}'.format(accuracy_score(y_test_encoded, predicted_s)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFMgDc2nN7cq","executionInfo":{"status":"ok","timestamp":1620890673535,"user_tz":-180,"elapsed":13316,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"13051415308691321255"}},"outputId":"9a31070b-ded1-43a0-f5f5-a0aa356651ba"},"source":["net = Net().cuda()\n","net.load_state_dict(torch.load(os.path.join(PATH, 'model_4000_480_800.txt')))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"qRWgG6LYpiTT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"48316f92-efc9-4ef8-fb1c-5c99ea88bf81"},"source":["import torch.optim as optim\n","\n","# net = Net().cuda()\n","\n","criterion = ContrastiveLoss(margin=800)\n","optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=0.0005)\n","\n","net.train()\n","\n","trainset = []\n","epoch_loss = 0\n","\n","for iteration in range(1000):\n","    del trainset\n","    trainset = shuffle_tensor_tuple(list(zip(train_images, train_labels)), 350)\n","\n","    if iteration % 200 == 0:\n","        epoch_loss = 0\n","\n","    for data in trainset:\n","        images, labels = data[0].cuda(), data[1].cuda()\n","\n","        optimizer.zero_grad()\n","\n","        labels_pred = net(*images)\n","        loss = criterion(*labels_pred, (labels[0] == labels[1]).type(torch.uint8))\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    if (iteration + 1) % 200 == 0:\n","        print(f'#{iteration + 1 + 4000}: {epoch_loss / 200}', end=' ')\n","        metric(net)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["#4200: 7261.62246467083 accuracy 0.361\n","#4400: 7157.441995723945 accuracy 0.348\n","#4600: 6907.213173335849 accuracy 0.361\n","#4800: 6832.002195410184 accuracy 0.358\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pDdqKo3u2Mhu"},"source":["torch.save(net.state_dict(), os.path.join(PATH, 'model_5000_480_800.txt'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HUE63YV1piTU"},"source":["Использование сиамской нейронной сети"]},{"cell_type":"code","metadata":{"id":"NkrwIawIu7o2"},"source":["def distance(x_1, x_2):\n","    return np.sqrt(np.sum((x_1 - x_2) ** 2, axis=1))\n","\n","net.eval()\n","\n","X_train_encoded = []\n","y_train_encoded = []\n","\n","trainset = shuffle_tensor(list(zip(train_images, train_labels)), 350)\n","\n","for images, labels in trainset:\n","    X_train_encoded.append(net.forward_once(images.cuda()).cpu().detach().numpy())\n","    y_train_encoded.append(labels.detach().numpy())\n","\n","X_train_encoded = np.reshape(np.array(X_train_encoded), (-1, 480))\n","y_train_encoded = np.reshape(np.array(y_train_encoded), (-1))\n","\n","\n","X_test_encoded = []\n","y_test_encoded = []\n","\n","testset = shuffle_tensor(list(zip(test_images, test_labels)), 300, 6)\n","\n","for images, labels in testset:\n","    X_test_encoded.append(net.forward_once(images.cuda()).cpu().detach().numpy())\n","    y_test_encoded.append(labels.detach().numpy())\n","\n","X_test_encoded = np.reshape(np.array(X_test_encoded), (-1, 480))\n","y_test_encoded = np.reshape(np.array(y_test_encoded), (-1))\n","\n","\n","predicted_s = []\n","\n","for encoded in X_test_encoded:\n","    dists = distance(encoded, X_train_encoded)\n","    index = dists.argmin()\n","\n","    predicted_s.append(y_train_encoded[index])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJK0QXn0piTV"},"source":["Вычисление метрик"]},{"cell_type":"code","metadata":{"id":"zGqq25fDpiTV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620858840465,"user_tz":-180,"elapsed":524,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"13051415308691321255"}},"outputId":"249801f5-5432-443c-843b-d6cfc916ef66"},"source":["print('accuracy {:.3}'.format(accuracy_score(y_test_encoded, predicted_s)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy 0.36\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qFGQc7rFIbMV"},"source":["# Сравнение результатов работы сиамской нейронной сетей с рассмотренными ранее нейронными сетями для полного датасета Omniglot (полный датасет)"]},{"cell_type":"code","metadata":{"id":"E8eSXXkJcm-H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620858842659,"user_tz":-180,"elapsed":526,"user":{"displayName":"Yauheni Hankovich","photoUrl":"","userId":"13051415308691321255"}},"outputId":"10b197cf-5b9b-4bc1-aead-2d9ae41481d6"},"source":["from sklearn.metrics import accuracy_score\n","\n","print('Siamese Neural network accuracy: {:.3}'.format(accuracy_score(y_test_encoded, predicted_s)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Siamese Neural network accuracy: 0.36\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pUiuyl4jJQtO"},"source":[""]}]}